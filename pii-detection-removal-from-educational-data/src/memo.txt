v1.2
split input to make the maximum length of input 1024


v1.3
downsample


v2.0
optuna
    1. simply create objective function of optuna with deberta-v3-base
        1. only learning_rate
        2. list up all the parameters to be optimized
            . lr
            . optimizer
            . drop_out
            . decay
            . batch size
            . epoch
            . fold
            . max_length

    2. is my optuna including early stopping?
        https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/003_efficient_optimization_algorithms.html
        https://www.youtube.com/watch?v=P6NwZVl8ttc 20:40
        the keyword is prune, should_prune, report
    3. classificate the process of optuna to enable the optuna to select the best parameter whenever I want to



v3.0
create a loss function suitable for the objective function f5
b5 threthold 0.9
    able to create a simple custom loss function
    know the restriction of the custom loss function
    imprement the f5 loss function meeting the restriction
    不均衡データ 適切な重み付け　または　訓練データを均衡にしする　適切な重み付けをしたときの訓練データの均衡度の調整が必要になったりする？   



v4.0
create new dataset to make oputuna to select the best parameter more practically
    https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/472221
    https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/483058
    https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/480747
    https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/477989
    https://www.kaggle.com/competitions/pii-detection-removal-from-educational-data/discussion/478911
then, optuna again



v5.0
enable google colab to use optuna



v6.0
read the tips for improve the model in LLM competitions
consider trailing_whitespace
LSTM
/n/n -> to some special token like [KAIGYO]
" " -> to some special token like [SPACE] ?
avarage the result of each fold
weighted avarage the result of each fold
input min, av, max, std to LGBM



v6.5
optuna again
    include early stopping
    classificate the process of optuna to enable the optuna to select the best parameter whenever I want to
    found the best
        . lr
        . optimizer
        . drop_out
        . decay
        . batch size
        . epoch
        . fold
        . max_length
    


v7.0
find the other model to ansamble
    gemma












MEMO:
batch_size * steps = total number of the data
step size is defined by the batch size automatically
the step count is different between P100 and T4x2 because the batch size is different
how to define the batch size?
if I can define the batch size manually, the step size for the P100 and T4x2 will be the same
P100: 2553 steps, 2553 / 3 = 851 (steps), 6807 / 851 = 8  (batch size)
T4x2: 1278 steps, 1278 / 3 = 426 (steps), 6807 / 426 = 16 (batch size)
epoch = the number of learning using the whole data
T4x2: 1278 / 3 = 426 (steps), 6807 / 426 = 16 (batch size)
input_ids
    X. 260
    .  323
    X, 261  
    ,  366
    X; 346 
    ;  2600

