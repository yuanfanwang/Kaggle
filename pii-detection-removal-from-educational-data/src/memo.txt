# v0.1 TODO:
# (done)        epoch (the size is 3 as default?)
# (done)        batch (fix the size as 8 or 16)

# v0.2:
# (done)        AutoModelForTokenClassification

# v0.3:
# the relation between the token size and the hidden size, currently the hidden size is 768
# model_config
# check why torch.cuda.empty_cache() doesn't work

# v0.4:
# (done)        metrics
# loss

# v0.5:
# fold, how to define the fold

# v0.6:
# (done)        change dummy labels to real labels
# (done)        practical metrics func
# practical loss func

# v1.0:
# sample submission


# v2.0:
# create new dataset to train the model strongly
# use google colab to train the model using devera-v3-xlarge
# optuna?

# v3.0:
# parameter tuning
# find the other model

# MEMO:
# batch_size * steps = total number of the data
# step size is defined by the batch size automatically
# the step count is different between P100 and T4x2 because the batch size is different
# how to define the batch size?
# if I can define the batch size manually, the step size for the P100 and T4x2 will be the same
# P100: 2553 steps, 2553 / 3 = 851 (steps), 6807 / 851 = 8  (batch size)
# T4x2: 1278 steps, 1278 / 3 = 426 (steps), 6807 / 426 = 16 (batch size)
# epoch = the number of learning using the whole data

# T4x2: 1278 / 3 = 426 (steps), 6807 / 426 = 16 (batch size)